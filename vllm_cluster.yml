---
- name: Deploy VLLM Cluster
  hosts: all
  become: true
  tasks:

    - name: Create run_cluster.sh script
      copy:
        content: |
          #!/bin/bash

          if [ $# -lt 4 ]; then
              echo "Usage: $0 docker_image head_node_address --head|--worker path_to_hf_home [additional_args...]"
              exit 1
          fi

          DOCKER_IMAGE="$1"
          HEAD_NODE_ADDRESS="$2"
          NODE_TYPE="$3"
          PATH_TO_HF_HOME="$4"
          shift 4
          ADDITIONAL_ARGS=("$@")

          if [ "${NODE_TYPE}" != "--head" ] && [ "${NODE_TYPE}" != "--worker" ]; then
              echo "Error: Node type must be --head or --worker"
              exit 1
          fi

          # cleanup() {
          #     docker stop node
          #     docker rm node
          # }
          # trap cleanup EXIT

          RAY_START_CMD="ray start --block"
          [ "${NODE_TYPE}" == "--head" ] && RAY_START_CMD+=" --head --port=6379" || RAY_START_CMD+=" --address=${HEAD_NODE_ADDRESS}:6379"

          docker run -d --entrypoint /bin/bash --network host --name node --shm-size 10.24g --gpus all \
              -v "${PATH_TO_HF_HOME}:/root/.cache/huggingface" "${ADDITIONAL_ARGS[@]}" "${DOCKER_IMAGE}" -c "${RAY_START_CMD}"
        dest: /root/run_cluster.sh
        mode: '0755'

    - name: Ensure no existing container with name "node" is running
      shell: |
        if docker ps -a --filter "name=node" -q; then
          docker stop node || true
          docker rm node || true
        fi
      ignore_errors: yes

    - name: Start Ray session
      shell: |
        bash /root/run_cluster.sh vllm/vllm-openai:v0.7.3 {{ groups['head'][0] }} {{ '--head' if inventory_hostname in groups['head'] else '--worker' }} /root/.cache/huggingface --privileged -e NCCL_IB_HCA='mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7' -e VLLM_HOST_IP={{ groups['head'][0] if inventory_hostname in groups['head'] else groups['workers'][0] }} -e USE_RDMA='true'
      
    - name: Start VLLM on head node
      shell: |
        docker exec -d node /bin/bash -c 'NCCL_DEBUG=TRACE nohup vllm serve deepseek-ai/DeepSeek-R1 \
        --tensor-parallel-size 16 --pipeline-parallel-size 1 --trust-remote-code \
        --max-model-len 32768 --gpu-memory-utilization 0.9 --enable-prefix-caching > /root/vllm.log 2>&1 &'
      when: inventory_hostname in groups['head']

